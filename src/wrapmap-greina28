#!/bin/sh
# GPUMAP
# GPU Socket Affinity wrapper for Slurm
# Written for CSCS by Mark Klein (klein@cscs.ch)
# For the MeteoSwiss System
# 1st of June 2015

export LOCAL_RANK=${SLURM_LOCALID}
PATH=/users/kleinm/hwloc/bin:$PATH
MYCORE=$(taskset -pc $$|awk '{print $6}'|awk -F, '{print $1}')
MYSOCKET=$(hwloc-calc --physical $(hwloc-bind --get) -I socket)

export MV2_ENABLE_AFFINITY=${MV2_ENABLE_AFFINITY-0}

assigngpu() {
	#All GPUs assigned to this job
	IFS="," read -a GPUS <<< "$GPU_DEVICE_ORDINAL"
	IFS="," read -a CPUS <<< "$JOBCORES"
	#Populate a map of assigned CPUs for job on node
	CPUSOCK1=()
	CPUSOCK2=()
	for i in ${CPUS[@]}
	do
		if [[ $i -lt 20 && $i -ge 10 ]] || [[ $i -ge 30 ]] 
		then
			CPUSOCK2+=($i)
		else
			CPUSOCK1+=($i)
		fi
	done
	#Populate a map of assigned GPUs for job on node
	GPUSOCK1=()
	GPUSOCK2=()
	for i in "${GPUS[@]}"
        do
                if [[ $i -lt 1 ]]
                then
                        GPUSOCK1+=($i)
                else
                        GPUSOCK2+=($i)
                fi
	done
		 # Rebalance the GPUS
	        if [[ ${#GPUSOCK1[@]} -gt ${#CPUSOCK1[@]} ]]
	        then
			NEEDED2=$(( ${#CPUSOCK2[@]} - ${#GPUSOCK2[@]}))
			if [[ $NEEDED2 -lt 0 ]]
			then
				NEEDED2=0
			fi
			GPUSOCK2+=(${GPUSOCK1[@]:0:$NEEDED2})
                        GPUSOCK1=(${GPUSOCK1[@]:$NEEDED2})
                fi
		if [[ ${#GPUSOCK2[@]} -gt ${#CPUSOCK2[@]} ]]
		then
                        NEEDED1=$(( ${#CPUSOCK1[@]} - ${#GPUSOCK1[@]}))
			if [[ $NEEDED1 -lt 0 ]]
			then
				NEEDED1=0
			fi

			GPUSOCK1+=(${GPUSOCK2[@]:0:$NEEDED1})	
			GPUSOCK2=(${GPUSOCK2[@]:$NEEDED1})
		fi

	#Task needs to know which set it belongs to
	if [[ $MYSOCKET -eq 0 ]]
	then
		CPUSOCK=(${CPUSOCK1[@]})
		GPUSOCK=(${GPUSOCK1[@]})
	else
		CPUSOCK=(${CPUSOCK2[@]})
		GPUSOCK=(${GPUSOCK2[@]})
	fi
	#Which Core Index am I?
	CI=0
	for n in "${CPUSOCK[@]}"; do
		if [[ $n -ne $MYCORE ]]
		then
			((CI++))
		else
			break
		fi
	done
	#We assign the GPUs in core order per socket
	#Leftovers get noDevFile (slurm gres spec)
	CUDADEV=${GPUSOCK[$CI]:-"NoDevFile"}
	CUDA=1
	export MV2_USE_CUDA=${MV2_USE_CUDA-1}
	export MV2_CUDA_IPC=${MV2_CUDA_IPC-0}
	export MV2_USE_GPUDIRECT=${MV2_USE_GPUDIRECT-1}
	export MV2_COMM_WORLD_LOCAL_RANK=${SLURM_LOCALID}
	export LOCAL_RANK=${SLURM_LOCALID}
	export MV2_GPUDIRECT_GDRCOPY_LIB=${MV2_GPUDIRECT_GDRCOPY_LIB-/cm/shared/apps/gdrcopy/libgdrapi.so}
	export CUDA_VISIBLE_DEVICES=${CUDA_FORCE_DEVICES-$CUDADEV}
	CI1=0
	CI2=0

	if [ $G2G -eq 2 ]
	then
		export MV2_CUDA_IPC=${MV2_CUDA_IPC-1}
		# build C_V_D order
		IFS="," read -a COREORDER <<< "$SLURM_CPU_BIND_LIST"
		for i in ${COREORDER[@]}
		do
			SOCK=$(hwloc-calc --physical -I socket $i)
			if [[ $SOCK -eq 0 ]]
			then
				GPULIST=$GPULIST,${GPUSOCK1[$CI1]}
				CI1=$((CI1+1))
			else
				GPULIST=$GPULIST,${GPUSOCK2[$CI2]}
				CI2=$((CI2+1))	
			fi
		done


		if [[ $CI1 -lt ${#GPUSOCK1[@]} ]]
		then
			for ((i=${CI1};i<${#GPUSOCK1[@]};i++))
			do
				GPULIST=$GPULIST,${GPUSOCK1[$i]}
			done
		fi
		if [[ $CI2 -lt ${#GPUSOCK2[@]} ]]
		then
			for ((i=${CI2};i<${#GPUSOCK2[@]};i++))
			do
				GPULIST=$GPULIST,${GPUSOCK2[$i]}
			done
		fi

				
		if [[ ${GPUSOCK[$CI]} ]] 
		then
			export CUDA_VISIBLE_DEVICES=${CUDA_FORCE_DEVICES-${GPULIST#?}}
		else
			export CUDA_VISIBLE_DEVICES=${GPU_DEVICE_ORDINAL}
		fi
	fi
	if [[ $G2G -eq 3 ]]
	then
                export MV2_CUDA_IPC=${MV2_CUDA_IPC-1}
                export OMP_DEFAULT_DEVICE=$((${SLURM_LOCALID} % ${#GPUS[@]}))
                export CRAY_ACC_DEVICE=$((${SLURM_LOCALID} % ${#GPUS[@]}))
		export CUDA_VISIBLE_DEVICES=${GPU_DEVICE_ORDINAL}
		export ACC_DEVICE_NUM=$((${SLURM_LOCALID} % ${#GPUS[@]}))
	fi
}
if [[ ! -z $G2G ]]
then
	CPULIST=$(echo $SLURM_CPU_BIND_LIST|tr "," " ")
	export CPULIST=$CPULIST
	if [[ ! -z $CPULIST ]]
	then
		JOBCORES=$(hwloc-calc --physical -I PU $CPULIST)
		JOBSOCKETS=$(hwloc-calc --physical -I socket $CPULIST)
	else
		JOBCORES=0
		JOBSOCKETS=0
	fi
	export JOBCORES=$JOBCORES
	assigngpu
fi

$@
